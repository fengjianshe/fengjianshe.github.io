<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Feng, Jianshe</title>
    <link>/</link>
      <atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    <description>Feng, Jianshe</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Fri, 08 May 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/images/icon_hu2da6660f49e0ed3d134821ae9bbede5e_267278_512x512_fill_lanczos_center_2.png</url>
      <title>Feng, Jianshe</title>
      <link>/</link>
    </image>
    
    <item>
      <title>Adaptive PHM Framework</title>
      <link>/project/6-adaptive-phm/</link>
      <pubDate>Fri, 08 May 2020 00:00:00 +0000</pubDate>
      <guid>/project/6-adaptive-phm/</guid>
      <description>&lt;p&gt;Prognostics and health management (PHM) has gradually become an essential technique to improve the availability and efficiency of industrial systems. With the rapid advancement
of sensor technology and communication technology, a huge amount of real-time data is generated from various applications industry, which brings new challenges to PHM in the context of big data streams. On one hand, high-volume stream data places a heavy demand on data storage, communication, and PHM modeling. On the other hand, continuous fluctuation and drift are essential properties of stream data in an online environment, which requires the PHM model to be capable to capture the new formation in stream data adaptively and continuously. This research proposes a systematic methodology to develop an effective online evolving PHM method with adaptive sampling mechanism against continuous stream data and in-process changes.&lt;/p&gt;
&lt;p&gt;An adaptive sample selection strategy named Sample Importance (SI) Test is developed to effectively select the representative samples in both off-line and online environment. The SI test proposed here is composed of two tests: Data-based importance test and Model-based importance test.
DBI test considers determining the informativeness of the new sample based on the characteristics of the data. In other words, to make it general, we define the DBI test as freshness test, as it evaluate the importance regarding informativeness just as it is &amp;ldquo;fresh&amp;rdquo; or not compared with previously shown samples. MBI test considers determining the informativeness of the new sample based on the performance of the model’s prediction on this sample. The prediction performance here has two meanings: (1) whether the prediction has a good prediction accuracy; and (2) whether the prediction has a low uncertainty. In other words, to make it general, we define the MBI test as error test, which identified whether the sample has a large prediction error, or has a high prediction uncertainty.&lt;/p&gt;
&lt;p&gt;Meanwhile, a probabilistic theory-based modeling approach is developed to update the model with newly selected samples. Finally, the whole methodology is validated with real-world industrial cases. The result comparison between the proposed methodology and state-ofart methods verifies the superiority of the proposed method.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Semi-automated FDC for Semiconductor Manufacturing Processes</title>
      <link>/project/4-semi-automated-fdc-for-semiconductor-manufacturing-processes/</link>
      <pubDate>Tue, 17 Dec 2019 00:00:00 +0000</pubDate>
      <guid>/project/4-semi-automated-fdc-for-semiconductor-manufacturing-processes/</guid>
      <description>&lt;p&gt;Feature-based univariate analysis (UVA) is the primary method for process monitoring and fault detection (FD) in today’s microelectronics manufacturing facilities. However, FD fails to use process variables correlations to detect fault and quality issues. Multivariate analysis (MVA) captures the correlation between different parameters in process; however, current MVA techniques such as principal component analysis (PCA), partial least squares (PLS), and their variants focus on threshold setting based on the principal or independent components extracted from the raw data, which practically cannot provide a direct limit setting on raw parameters for decision making support during online process monitoring. Also, in bypassing feature identification and extraction, the subject matter expert (SME) is largely left out of the loop because particular features can often be associated with particular fault types. Furthermore, it is difficult to visualize the multivariate limits due to the high dimensionality of the data produced in semiconductor manufacturing processes. Finally, slow and normal process changes often occur in real processes, which can lead to false alarms during implementation when using models trained from offline samples. There is a need for an FD technique that (1) leverages the existing base of feature-based UVA, (2) provides a mechanism to determine correlation among parameters, (3) provides a mechanism to understand and as necessary reject normal process drift, (4) provides for automatic calculation of limits UVA features that captures correlation among parameters, and (5) provides a method for simple viewing of these capabilities so that the subject matter expert (SME) can visualize, understand, and continue to contribute to the FD optimization process.&lt;/p&gt;
&lt;p&gt;In this work, we propose a systematic adaptive limit setting and visualization method for FD in semiconductor manufacturing processes that addresses the above 5 requirements. The proposed method has been shown effective when applied to a public dataset and datasets generated by our trace simulation tool.&lt;/p&gt;
&lt;p&gt;A high-level overview of the proposed methodology is described in Fig. 1, which mainly includes three major steps:
Step 1 - Pattern extraction and feature determination from traces: A set of trace data from wafer runs is used to learn the patterns and determine the features contained in the traces. Different patterns in the traces are detected by our pattern extraction algorithm. The trace segmentation techniques in this step further improve on the initial achievements that have been presented at the 28th, 29th, and 30th APC conferences [2][3][4]. Within each trace segment, expert knowledge is leveraged to finalize the feature set of interest. For example, the patterns such as ramp and flat are recognized by the pattern extraction processing. Either every single pattern or the combination of different contingent patterns will be taken as features of interest for further analysis.&lt;/p&gt;
&lt;p&gt;Step 2 - Feature parameter extraction and selection: The major task of Step 2 is to determine the distribution of parameters that describe the features determined in Step 1 and select the important parameters. The statistical parameters are mainly used to summarize the important information in the recognized trace patterns. For example, the slope, length, width, and height are extracted to depict a ramp feature; the analysis from Step 2 might determine the average of the slope for a feature across the provided data traces. In addition, the solution provides a method for an SME to be employed to depict and/or fine tune the properties of certain features.&lt;/p&gt;
&lt;p&gt;After the feature parameter extraction is completed, the important features are selected based on criteria, such as receiver operating characteristic (ROC) curve analysis. This selection can occur in an unsupervised setting (like traditional FD where quality data is not directly tied to the trace data) or in a supervised setting where quality data, such as yield or fault classification data, is directly tied to the trace data.&lt;/p&gt;
&lt;p&gt;Step 3 - FD model development and adaptive multivariate limit setting: Once the important feature parameters are selected, the FD model and corresponding multivariate limit setting mechanism are developed. In the analysis pipeline in Step 3, a check is performed first to find out if dominant feature parameters exist that fully differentiate faulty from healthy traces individually, which is equivalent to finding a suitable parameter for UVA. If no decisive parameters are found, multiple parameters are selected based on their importance ranking obtained in Step 2, and the multivariate FD model is established. To realize better visualization, understanding and decision-making (SME) integrated support, the multivariate limit determined by the FD model is projected and visualized in lower-dimensional views. In addition, considering the data drift in processes, the model adaptation mechanism is incorporated to address any data drift in processes and treat it as anomalous or normal (with this determination provided by the SME). The adaptation mechanism incorporates moving window, data filtering, and incremental modelling methods.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>CPS-Enabled Rehabilitation System for Improved Patient Recovery</title>
      <link>/project/5-cps-enabled-rehabilitation-system-for-improved-patient-recovery-/</link>
      <pubDate>Mon, 17 Dec 2018 00:00:00 +0000</pubDate>
      <guid>/project/5-cps-enabled-rehabilitation-system-for-improved-patient-recovery-/</guid>
      <description>&lt;p&gt;A Cyber-Physical System (CPS)-enabled rehabilitation system framework for enhanced recovery rate in gait training systems is presented in this paper. Recent advancements in sensing and data analytics have paved the way for the transformation of healthcare systems from experience-based to evidence-based. To this end, this paper introduces a CPS-enabled rehabilitation system which collects, processes and models the data from patient and rehabilitative training machines. This proposed system consists of a set of sensors to collect various physiological data as well as machine parameters. The sensors and data acquisition systems are connected to an edge computing unit that handles the data preprocessing, analytics and results visualization. Advanced machine learning algorithms are used to analyze data from physiological data, machine parameters and patients’ metadata to quantify each patient’s recovery progress, devise personalized treatment strategies, adjust machine parameters for optimized performance, and provide feedback regarding patient’s adherence to instructions. Moreover, the accumulation of the knowledge gathered by patients with different conditions can provide a powerful tool for better understanding the human-machine interaction and its impact on patient recovery. Such system can eventually serve as a ‘Virtual Doctor’, providing accurate feedback and personalized treatment strategies for patients.&lt;/p&gt;
&lt;p&gt;The proposed CPS-enabled rehabilitation system leverages the Prognostics &amp;amp; Health Management (PHM) technologies in the engineering field, which provides insight into machinery equipment’s current performance and the estimated remaining useful life before failure based on advanced data analytics. Similarly, in the rehabilitation field, data analytics can be used to assess patients’ current recovery condition as well as estimate the remaining rehabilitative time before full recovery. A comprehensive framework of CPS-enabled rehabilitation system is described in Figure 1. The framework includes three primary modules: (1) Edge computing module. As an edge server, it is dedicated to collect physiological signals, perform data pre-processing, feature extraction, health diagnosis and prognosis. (2) Healthcare cloud module. The healthcare cloud module provides the services for information storage and analytics tools for patient recovery profiles, features, and health models. (3) User interface module. The user interface module is a user-friendly online graphical dashboard which is reconfigurable so that patients and doctors can retrieve different information such as training plan, recovery progress tracking (for patient), and patient management, feedback interactions (for doctor).&lt;/p&gt;
&lt;p&gt;As a summary, a CPS-enabled framework of rehabilitation system for improved patient recovery is proposed. The CPS-based rehabilitation platform carves out the vision and practical guidelines in the healthcare area to implement CPS for better human monitoring and rehabilitation training process. The future work will include deployment to realize a seamless connection among rehabilitation training, health prognosis, and rehabilitation recovery decision making.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>PHM-enabled Maintenance Scheduling Optimization</title>
      <link>/project/3-phm-enabled-maintenance-scheduling-optimization/</link>
      <pubDate>Sun, 17 Dec 2017 00:00:00 +0000</pubDate>
      <guid>/project/3-phm-enabled-maintenance-scheduling-optimization/</guid>
      <description>&lt;p&gt;Maintenance Scheduling and Routing (MS&amp;amp;R) is critical for the offshore wind farm to reduce maintenance cost. Although different models are proposed, the turbine operating conditions and the forecasted wind resources in the maintenance horizon are still less accounted in these current models. To address this issue, this research proposes a novel mathematical model to optimize the MS&amp;amp;R problem by highlighting the significance of turbine production loss (PL) before and during maintenance activities. In the proposed methodology, the PL term takes the most up-to-date wind turbine power curve and the forecasted wind resources as model inputs. Subsequently, a novel Genetic Algorithm (GA) solver is designed to minimize the PL of wind turbines together with the technician salaries and the transportation costs. The outcome of the proposed model gives a detailed maintenance plan with maintenance schedules, vessel routes, technician assignments, and cost breakdowns. Validation of the proposed model is implemented on real-world data collected from an offshore wind farm with several 4 MW wind turbines. The result demonstrates the effectiveness and superiority of the proposed method, and some practical findings are also summarized in the conclusions.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Battery SoC Prognosis</title>
      <link>/project/2-battery-soc-prognosis/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      <guid>/project/2-battery-soc-prognosis/</guid>
      <description>&lt;p&gt;State of Capacity (SoC), State of health (SoH) forecasts and RUL prediction are increasingly important in battery prognostics as they are key parameters of appropriate battery management strategy to avoid catastrophic failure, to enhance battery durability and to optimize cost. Also, due to the widespread applications of Lithium-ion Batteries (LiB) in many industrial sectors, research of SoH, SoC prediction holds great academic value and economic impact. In general, SoH is defined as a performance index to describe the degree of degradation of battery, and SoC denotes capacity of battery in compared to the capacity in its fully charged state. In the present study, SoC, SoH, and RUL prediction mainly predicts future battery capacity, which helps battery to be used to designed potential and maximum life expectancy before failed.&lt;/p&gt;
&lt;p&gt;To present an efficient, adaptive, and accurate solution for battery capacity prediction in a multi-cell setting, this study aims to build up an online battery capacity prognosis solution. The solution demonstrates an efficient yet effective way to exploit the cross-trajectory correlations without adding many computation complexities to the standard GPR model, but taking advantages of the historical available data.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Online Virtual Metrology of Semiconductor Manufacturing Processes</title>
      <link>/project/1-online-virtual-metrology-of-semiconductor-manufacturing-processes/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      <guid>/project/1-online-virtual-metrology-of-semiconductor-manufacturing-processes/</guid>
      <description>&lt;p&gt;Virtual Metrology (VM) is an essential research for manufacturing processes, which aims to estimate the difficult-to-measure parameters of products or processes by computer programs. Modeling of dynamic manufacturing processes with slow drift using data-driven approaches is challenging because most data-driven models are trained by off-line data. In this paper, we propose to track slow drift of manufacturing process using an online Bayesian Auto-Regression eXogenous (ARX) model with time-variant parameters. The model parameters are trained off-line and are updated online using Bayes’ rule. To avoid frequent online model update, a Sample Importance test is proposed to screen online samples and only the sample that passes SI test is incremented to the VM model. The SI test decides the importance of the sample by collectively considering sample freshness, model prediction error and prediction uncertainty. The off-line sample selection algorithm can effectively reduce the sample redundancy in off-line DB while maintaining the model performance. To validate the effectiveness of the proposed method, Prognostics and Health Management (PHM) data challenge 2016 dataset is employed to predict material removal rate of chemical-mechanical planarization process. The validation results indicate that the proposed method outperforms existing state-of-art approaches in literature such as Just-in-Time (JIT) and deep learning models.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
